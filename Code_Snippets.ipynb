{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Snippets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sksoumik/programming_notes/blob/main/Code_Snippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXJOJPL-Qgpm"
      },
      "source": [
        "## Basic library imports for colab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B1ltShnQYT9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9372194b-ad60-42b2-94a1-76187f56853f"
      },
      "source": [
        "# mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# force output to display the full text \n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch \n",
        "\n",
        "\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "# print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmyBMKHNP3N_"
      },
      "source": [
        "## Filter dataframe rows with string lenght "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzROR1aYP_Vq"
      },
      "source": [
        "df = df[df['text'].str.split().str.len() > 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bqdLV5fSTXe"
      },
      "source": [
        "## Text data cleaning methods, nlp preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrEZFrBwS_mZ"
      },
      "source": [
        "import emoji\n",
        "import re\n",
        "import html\n",
        "import unicodedata\n",
        "import unidecode\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    text = standardize_text(text)\n",
        "    text = replace_url(text)\n",
        "    text = replace_mail_or_mentions(text)\n",
        "    text = process_emojis(text)\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "\n",
        "def standardize_text(text):\n",
        "    \"\"\"\n",
        "    1) Escape HTML\n",
        "    2) Replaces some non-standard punctuation with standard versions.\n",
        "    3) Replace \\r, \\n and \\t with white spaces\n",
        "    4) Removes all other control characters and the NULL byte\n",
        "    5) Removes duplicate white spaces\n",
        "    \"\"\"\n",
        "    # escape HTML symbols\n",
        "    text = html.unescape(text)\n",
        "    # standardize punctuation\n",
        "    # translate table for punctuation\n",
        "    transl_table = dict([(ord(x), ord(y))\n",
        "                         for x, y in zip(u\"â€˜â€™Â´â€œâ€â€“-\",  u\"'''\\\"\\\"--\")])\n",
        "    text = text.translate(transl_table)\n",
        "    text = text.replace('â€¦', '...')\n",
        "    # replace \\t, \\n and \\r characters by a whitespace\n",
        "    control_char_regex = re.compile(r'[\\r\\n\\t]+')\n",
        "    text = re.sub(control_char_regex, ' ', text)\n",
        "    # remove all remaining control characters\n",
        "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n",
        "    # replace multiple spaces with single space\n",
        "    text = ' '.join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def standardize_punctuation(text):\n",
        "    return ''.join([unidecode.unidecode(t) if unicodedata.category(t)[0] == 'P' else t for t in text])\n",
        "\n",
        "\n",
        "def replace_url(text, replace_token=\"\"):\n",
        "    url_regex = re.compile(r'((www.\\S+)|(https?://\\S+))')\n",
        "    text = re.sub(url_regex, replace_token, text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def process_emojis(text, replace_token=\"\", remove=False,):\n",
        "    def remove_emojis(text, replace_token=\"\"):\n",
        "        text = \"\".join(\n",
        "            [c if c not in emoji.UNICODE_EMOJI else replace_token for c in text])\n",
        "        return text\n",
        "\n",
        "    def asciify_emojis(text):\n",
        "        \"\"\"Convert emojis into text aliases.\n",
        "        E.g. ðŸ‘ -> :thumbs_up: -> :thumbsup:\n",
        "        \"\"\"\n",
        "        text = emoji.demojize(text)\n",
        "        emojis = re.findall(r'\\:(.*?)\\:', text)\n",
        "        for e in emojis:\n",
        "            text = text.replace(e, ''.join(e.split('_')))\n",
        "        return text\n",
        "\n",
        "    if remove:\n",
        "        return remove_emojis(text, replace_token=\"\")\n",
        "    return asciify_emojis(text)\n",
        "\n",
        "\n",
        "def replace_mail_or_mentions(text, replace_token=\"\"):\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', replace_token, text)\n",
        "    return text\n",
        "\n",
        "##########################################################################\n",
        "######## Other options that might be helpful to preprocess ###############\n",
        "##########################################################################\n",
        "\n",
        "# remove all rows that contain any non-ascii characters\n",
        "\n",
        "df['text'] = df[~df.text.str.contains(r'[^\\x00-\\x7F]', na=False)]\n",
        "\n",
        "# remove nan and duplicates \n",
        "df = df[df['text'].notnull()]\n",
        "df.drop_duplicates(keep=False, inplace=True)\n",
        "\n",
        "# remove URLs\n",
        "def remove_url(row):\n",
        "    new_text = re.sub(r'http\\S+', '', row)\n",
        "    return new_text\n",
        "\n",
        "\n",
        "# remove contents that are inside <*>\n",
        "\n",
        "def remove_contents_in_brace(row):\n",
        "    new_text = re.sub(r'<.*>', '', row)\n",
        "    return new_text \n",
        "\n",
        "\n",
        "# remove double spaces \n",
        "def remove_double_space(sentence): \n",
        "    new_sentence = \" \".join(sentence.split())\n",
        "    return new_sentence\n",
        "  \n",
        "\n",
        "  # remove `, ` from the beginning of a sentence. \n",
        "def remove_leading_punc(row):\n",
        "    row = row[2:] if row.startswith(', ') else row\n",
        "    return row \n",
        "\n",
        "  \n",
        "# remove all special characters - punctuations \n",
        "df['text'] = df['text'].str.replace(r'[^\\w\\s]+', '')\n",
        "  \n",
        "  \n",
        "# remove digits\n",
        "import string\n",
        "\n",
        "df['text'] = df['text'].str.rstrip(string.digits)\n",
        "\n",
        "\n",
        "# remove words that contain number and character both: IDs\n",
        "# for example: U017Q2N13J \n",
        "def remove_numbers(words):\n",
        "    new_text = re.sub(r'\\w*\\d\\w*', '', words).strip()\n",
        "    return new_text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTkaoyTYTkRX"
      },
      "source": [
        "## Remove null and duplicates from pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5N69_GrTqER"
      },
      "source": [
        "# remove nan and duplicates \n",
        "df = df[df['text'].notnull()]\n",
        "df.drop_duplicates(keep=False, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I15HoZZXTsbZ"
      },
      "source": [
        "## Average sentence length in a column of dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLfjCCssT8ID"
      },
      "source": [
        "print('average sentence length: ', df.Text.str.split().str.len().mean())\n",
        "print('stdev sentence length: ', df.Text.str.split().str.len().std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn61Xtc5T-UT"
      },
      "source": [
        "## Display all images in a python List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcFMItjsUMZo"
      },
      "source": [
        "import cv2 as cv \n",
        "import glob\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "path = \"static/subfolder/*/*.jpg\"\n",
        "\n",
        "my_image_list = []\n",
        "\n",
        "for file in glob.glob(path):\n",
        "    file = cv.imread(file) # BGR\n",
        "    # convert BGR to RGB \n",
        "    rgb_image = cv.cvtColor(file, cv.COLOR_BGR2RGB)\n",
        "    my_image_list.append(rgb_image)\n",
        "\n",
        "# display all images \n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "columns = 4\n",
        "\n",
        "for i, image in enumerate(my_image_list):\n",
        "    plt.subplot(len(my_image_list) / columns + 1, columns, i + 1)\n",
        "    plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTsl_fHuURR6"
      },
      "source": [
        "## Plot number of values distribution in a column of dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J__rGTi4UhWB"
      },
      "source": [
        "data['target_column'].value_counts().plot.bar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYlgQKd1UlY6"
      },
      "source": [
        "## Balancing class distribution of a dataset: Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmcJfThsVBME"
      },
      "source": [
        "def sampling_k_elements(group, k=3):\n",
        "if len(group) < k:\n",
        "    return group\n",
        "return group.sample(k)\n",
        "\n",
        "balanced = df.groupby('class').apply(sampling_k_elements).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvFa8sZNVGuP"
      },
      "source": [
        "## Save the print output in a txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c57a7bxbVSzl"
      },
      "source": [
        "import sys\n",
        "\n",
        "# put your output data container\n",
        "print(report)\n",
        "\n",
        "original_stdout = sys.stdout\n",
        "\n",
        "with open(\"classification_report.txt\", \"w\") as f:\n",
        "    sys.stdout = f\n",
        "    # put your output data container again\n",
        "    print(report)\n",
        "    sys.stdout = original_stdout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdofGJTSVZqu"
      },
      "source": [
        "## Mapping one list to another: adding two lists together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7vySgxjVvln"
      },
      "source": [
        "\"\"\"\n",
        "categories: List[str]\n",
        "category_ids: List[int]\n",
        "\"\"\"\n",
        "\n",
        "label_details = list(map(lambda x, y: x+ ':' +str(y), categories, category_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_BADi9WVzE3"
      },
      "source": [
        "## Balancing class distribution is train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqgX-ND-gIhi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(data,\n",
        "                                    stratify=data['class_id'], \n",
        "                                    test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRLosR5HgMPe"
      },
      "source": [
        "## Evaluation report generation with true label and predicted labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5fsyJUigiWg"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# read evaluation data\n",
        "evaluation_df = pd.read_csv(\"path/to/evaluation/file.csv\") \n",
        "\n",
        "true_label = list(evaluation_df['class_id'])\n",
        "\n",
        "evaluation_texts = []\n",
        "\n",
        "for i in evaluation_df[\"Text\"]:\n",
        "    evaluation_texts.append(i)\n",
        "\n",
        "\n",
        "# evaluation scores using ml model\n",
        "prediction_probs = []\n",
        "\n",
        "for i in tqdm(evaluation_texts):\n",
        "    preds, _ = model.predict(evaluation_texts) # predict function, must return a score \n",
        "    prediction_probs.append(preds) \n",
        "\n",
        "# make a new dataframe using the target texts and prediction probabilities\n",
        "prediction_df = pd.DataFrame(\n",
        "    {\n",
        "        \"Comment\": evaluation_texts,\n",
        "        'true labels': true_label,\n",
        "        'Prediction' : prediction_probs,\n",
        "    }\n",
        ")\n",
        "\n",
        "# save the prediction df \n",
        "prediction_df.to_csv(\"save/path/filename.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqfOtAJRjCMf"
      },
      "source": [
        "## scikit learn confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1puFtLI7jHeU"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "true_label = list(evaluation_df['true labels'])\n",
        "predicted_labels = list(evaluation_df['predicted labels'])\n",
        "\n",
        "# y_test = true_label\n",
        "# y_pred = predicted_labels\n",
        "\n",
        "matrix = confusion_matrix(true_label, predicted_labels)\n",
        "matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh7Da2prjnMM"
      },
      "source": [
        "## Plot confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAc9W-3Jjqyt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Below code has been adapted from: https://github.com/DTrimarchi10/confusion_matrix\n",
        "\"\"\"\n",
        "def make_confusion_matrix(cf,\n",
        "                          group_names=None,\n",
        "                          categories='auto',\n",
        "                          count=True,\n",
        "                          percent=True,\n",
        "                          cbar=True,\n",
        "                          xyticks=True,\n",
        "                          xyplotlabels=True,\n",
        "                          sum_stats=True,\n",
        "                          figsize=None,\n",
        "                          cmap='Blues',\n",
        "                          title=None):\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
        "    blanks = ['' for i in range(cf.size)]\n",
        "\n",
        "    if group_names and len(group_names)==cf.size:\n",
        "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
        "    else:\n",
        "        group_labels = blanks\n",
        "\n",
        "    if count:\n",
        "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
        "    else:\n",
        "        group_counts = blanks\n",
        "\n",
        "    if percent:\n",
        "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
        "    else:\n",
        "        group_percentages = blanks\n",
        "\n",
        "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
        "    if sum_stats:\n",
        "        #Accuracy is sum of diagonal divided by total observations\n",
        "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
        "\n",
        "        #if it is a binary confusion matrix, show some more stats\n",
        "        if len(cf)==2:\n",
        "            #Metrics for Binary Confusion Matrices\n",
        "            precision = cf[1,1] / sum(cf[:,1])\n",
        "            recall    = cf[1,1] / sum(cf[1,:])\n",
        "            f1_score  = 2*precision*recall / (precision + recall)\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
        "                accuracy,precision,recall,f1_score)\n",
        "        else:\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
        "    else:\n",
        "        stats_text = \"\"\n",
        "\n",
        "\n",
        "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
        "    if figsize==None:\n",
        "        #Get default figure size if not set\n",
        "        figsize = plt.rcParams.get('figure.figsize')\n",
        "\n",
        "    if xyticks==False:\n",
        "        #Do not show categories if xyticks is False\n",
        "        categories=False\n",
        "\n",
        "\n",
        "    # MAKE THE HEATMAP VISUALIZATION\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
        "\n",
        "    if xyplotlabels:\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label' + stats_text)\n",
        "    else:\n",
        "        plt.xlabel(stats_text)\n",
        "    \n",
        "    if title:\n",
        "        plt.title(title)\n",
        "\n",
        "\n",
        "\n",
        "# call the function\n",
        "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
        "categories = ['label_name (0)', 'label_name (1)', 'label_name (2)', 'label_name (3)', 'label_name (4)', 'label_name (5)']\n",
        "make_confusion_matrix(matrix, \n",
        "                      group_names=labels,\n",
        "                      categories=categories, \n",
        "                      cmap='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkQHuxjZkMoY"
      },
      "source": [
        "## scikit learn classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePWZfvnpkXz-"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = true_label\n",
        "y_pred = predicted_labels\n",
        "target_names = ['label_name (0)', 'label_name (1)', 'label_name (2)', 'label_name (3)', 'label_name (4)', 'label_name (5)']\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk97RacvknkW"
      },
      "source": [
        "## Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjw_3buOkyRy"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZkIYH5Nkziz"
      },
      "source": [
        "## rename columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYZTqOZrlGgF"
      },
      "source": [
        "rename_cols = {\n",
        "    0: 'class_id',\n",
        "    1: 'comment'\n",
        "}\n",
        "\n",
        "df = df.rename(columns=rename_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWKY--XwlMJ_"
      },
      "source": [
        "## Convert text classes to numeric values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtXW05xmmknM"
      },
      "source": [
        "'''\n",
        "convert text classes to numeric value.\n",
        "e.g. \n",
        "positive -> 0\n",
        "negative -> 1\n",
        "neutral -> 2 \n",
        "etc.... \n",
        "'''\n",
        "\n",
        "\n",
        "def encode_class(data):\n",
        "    for i in range(len(data[\"class\"].unique())):\n",
        "        data.loc[data[\"class\"] == data[\"class\"].unique()[i], \"class_id\"] = i\n",
        "\n",
        "    data[\"class_id\"] = data[\"class_id\"].astype(\"Int64\")\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_03SSKmoq-"
      },
      "source": [
        "## Undersample and class balancing to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-prRNzOmsFv"
      },
      "source": [
        "'''\n",
        "Undersample and balance all classes' data points to avoid biasness. \n",
        "make all class's data points equal \n",
        "'''\n",
        "\n",
        "\n",
        "def sampling_train_data(\n",
        "    group,\n",
        "    k=int(\n",
        "        input(\n",
        "            \"Enter the amount of data that you want for each class for train set: \"\n",
        "        ))):\n",
        "\n",
        "    if len(group) < k:\n",
        "        return group\n",
        "    return group.sample(k)\n",
        "\n",
        "\n",
        "def sampling_test_data(\n",
        "    group,\n",
        "    k=int(\n",
        "        input(\n",
        "            \"Enter the amount of data that you want for each class for test set: \"\n",
        "        ))):\n",
        "\n",
        "    if len(group) < k:\n",
        "        return group\n",
        "    return group.sample(k)\n",
        "\n",
        "\n",
        "def balance_data(data):\n",
        "    train_data = data[data[\"train-test\"] == \"train\"]\n",
        "    test_data = data[data[\"train-test\"] == \"test\"]\n",
        "    train_df = train_data[[\"feature\", \"class_id\", \"class\"]]\n",
        "    test_df = test_data[[\"feature\", \"class_id\", \"class\"]]\n",
        "    train_df = train_df.groupby('class_id').apply(\n",
        "        sampling_train_data).reset_index(drop=True)\n",
        "    test_df = test_df.groupby('class_id').apply(\n",
        "        sampling_test_data).reset_index(drop=True)\n",
        "    # save train and test data to disk\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    train_df.to_csv(\"data/train_data.csv\", index=False)\n",
        "    test_df.to_csv(\"data/test_data.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkBmsUV4m-UF"
      },
      "source": [
        "## simpletransformers installation dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSYrTJslnSp_"
      },
      "source": [
        "!pip install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers==2.11.0\n",
        "!pip install simpletransformers==0.41.1\n",
        "!git clone --recursive https://github.com/NVIDIA/apex.git\n",
        "!cd apex && pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcQXjShsLR5k"
      },
      "source": [
        "## most frequent value row wise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPgr0DUGLQ10"
      },
      "source": [
        "df.mode(axis=1)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-UOk1PdMD-9"
      },
      "source": [
        "## multiple dataframe concate column wise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxdgHdl9MKQ4"
      },
      "source": [
        "df_list = [df1, df2, df3]\n",
        "new_df = pd.concat(df_list, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMor1Z0iMUZJ"
      },
      "source": [
        "## Remove empty/blank rows from dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3XX6UkMM-bQ"
      },
      "source": [
        "df['col_name'].replace('  ', np.nan, inplace=True)\n",
        "df = df.dropna(subset=['col_name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Rek-xbNHlX"
      },
      "source": [
        "## multi label target list conversion from normal columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0p264rROl0H"
      },
      "source": [
        "cols = ['col_name_1','col_name_2']\n",
        "train_df['Labels'] = train_df[cols].values.tolist()\n",
        "\n",
        "'''\n",
        "\n",
        "col_name_1   col_name_2    Labels\n",
        "-------------------------------------\n",
        "   0            0           [0, 0]\n",
        "   1            1           [1, 1] \n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0eLm6oxPiYN"
      },
      "source": [
        "## download data from drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmRxs_0UP0PJ"
      },
      "source": [
        "! gdown --id 1njvYa1P3ZVCzCDusvOZV7Z_4P "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DhuZp-rQALF"
      },
      "source": [
        "## int64 conversion of target values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-MzNljwP38i"
      },
      "source": [
        "df.loc[:, 'col_name'] = df['col_name'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyl-toL0QPi4"
      },
      "source": [
        "## shuffle rows in pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ju878ZDQay9"
      },
      "source": [
        "df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijv4g6mdQc9s"
      },
      "source": [
        "## text data augmentation nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NukbCZ9PQrRi"
      },
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as naf\n",
        "from nlpaug.util import Action\n",
        "\n",
        "\n",
        "# parameter list:\n",
        "# https://nlpaug.readthedocs.io/en/latest/augmenter/augmenter.html\n",
        "\n",
        "\n",
        "TOPK=20 \n",
        "ACT = 'insert'\n",
        "\n",
        "aug_distilbert = naw.ContextualWordEmbsAug(\n",
        "    model_path='distilbert-base-uncased', \n",
        "    #device='cuda',\n",
        "    action=ACT, top_k=TOPK)\n",
        "\n",
        "\n",
        "# keep the original data and create 3 augmented data using contexual embedding\n",
        "def contexual_embedding(text):\n",
        "    x = aug_distilbert.augment(text, n=3)\n",
        "    x += [text]\n",
        "    return x\n",
        "\n",
        "\n",
        "# this will keep the label columns as well \n",
        "df[\"col_name\"] = df[\"col_name\"].apply(lambda x: contexual_embedding(x)) \n",
        "\n",
        "# explode data\n",
        "dump_data = df.explode('col_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWVd-lZ1xO2r"
      },
      "source": [
        "## filter column values based on given data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R06JBUtJxThN"
      },
      "source": [
        "# removes all value rows other than 0/1 in col_1 and col_2\n",
        "df = df[df[['col_1','col_2']].isin([0,1]).all(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2f5IvpSxirF"
      },
      "source": [
        "## remove rows from one dataframe that exist in other dataframe based on index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtMD6kibx7lr"
      },
      "source": [
        "df2_indicies = df2.index.values.tolist()\n",
        "df1 = df1.drop(df1.index[df2_indicies])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3-heSkEyGRw"
      },
      "source": [
        ""
      ]
    }
  ]
}